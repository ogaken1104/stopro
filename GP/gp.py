import jax
import jax.numpy as jnp
import numpy as np
from jax import grad, jit, vmap


class GPmodel:
    def __init__(self, approx_non_pd: bool = False):
        self.approx_non_pd = approx_non_pd

    def cholesky_decompose_non_positive_definite(self, matrix, noise, **kwargs):
        """Compute cholesky decomposition with modifying non positive semedifinite matrix to positive semidifinite matrix"""
        w, v = jnp.linalg.eigh(matrix)
        w = jnp.where(w < 0, 1e-19, w)  # make this pd, psd is insufficient
        matrix_positive_definite = v @ jnp.eye(len(v)) * w @ v.T
        matrix_positive_definite_noise = matrix_positive_definite + jnp.diag(noise)

        return jnp.linalg.cholesky(matrix_positive_definite_noise), np.any(w < 0)

    def logpGP(self, Œ¥f, Œ£, œµ, approx_non_pd=False):
        """Compute minus log-likelihood of observing Œ¥f = f - <f>, for GP with covariance matrix Œ£"""
        n = len(Œ¥f)
        # jiggle parameter to improve numerical stability of cholesky decomposition
        noise = jnp.ones_like(Œ¥f) * œµ
        if approx_non_pd:
            ####### modify semidifinite to difinite ###############
            L, _is_non_pd = self.cholesky_decompose_non_positive_definite(Œ£, noise)
        else:
            ######### default ###################
            L = jnp.linalg.cholesky(Œ£ + jnp.diag(noise))
        v = jnp.linalg.solve(L, Œ¥f)
        return (
            0.5 * jnp.dot(v, v)
            + jnp.sum(jnp.log(jnp.diag(L)))
            + 0.5 * n * jnp.log(2.0 * jnp.pi)
        )

    def postGP(self, Œ¥fb, Kaa, Kab, Kbb, œµ, approx_non_pd=False):
        """Compute posterior average and covariance from conditional GP p(fa | xa, xb, fb)
        [fa,fb] ~ ùí©([Œº_fa, Œº_fb], [[Kaa, Kab],[Kab^T, Kbb]])])
        fa|fb   ~ ùí©(Œºf + Kab Kbb \ (fb - Œº_fb) , Kaa - Kab Kbb \ Kab^T)
        """
        noise = jnp.ones(len(Kbb)) * œµ
        if approx_non_pd:
            ################## modify semidifinite to definite ###############
            L, non_pd = self.cholesky_decompose_non_positive_definite(Kbb, noise)
        else:
            ################# default ######################
            L = jnp.linalg.cholesky(Kbb + jnp.diag(noise))

        # Œ± = K \ Œ¥ f = L^t \ (L | Œ¥ f)
        Œ± = jnp.linalg.solve(L.transpose(), jnp.linalg.solve(L, Œ¥fb))

        # Œºpost - Œº(x*) = Kab Kbb \ Œ¥f(x) = Kab . Œ±
        Œºpost = jnp.dot(Kab, Œ±)

        # Kpost = Kaa - Kab Kbb | Kab^T
        #       = Kaa - W
        # W_ij  = v_i . v_j
        # v_i   = (L | c_i) ; c_i the i-th column of Kba, i-th row of Kab
        V = jnp.array([jnp.linalg.solve(L, c) for c in Kab])  # V = [v_1, v_2, ... ]^t
        Kpost = Kaa - jnp.einsum("ik,jk->ij", V, V)
        return Œºpost, Kpost  # note should add Œº(x*) to average

    def calculate_K_symmetric(self, pts, Ks, std_noise_list=[None] * 8):
        """Compute symmetric part of covariance matrix (training_K and test_K)"""
        r_num = len(pts)

        sec = np.zeros(r_num + 1, dtype="int")
        r = []
        for i, x in enumerate(pts):
            r.append(x)
            sec[i + 1 :] += len(x)
        Œ£ = jnp.zeros((sec[r_num], sec[r_num]))
        for i in range(r_num):
            for j in range(i, r_num):
                # upper triangular matrix
                Œ£ = Œ£.at[sec[i] : sec[i + 1], sec[j] : sec[j + 1]].set(
                    Ks[i][j - i](r[i], r[j])
                )
                # if i == j and std_noise_list[i] is not None:
                #     var_noise_list = jnp.full(len(r[i]), std_noise_list[i]**2)
                #     var_noise_matrix = jnp.diag(var_noise_list)
                #     Œ£ = Œ£.at[sec[i]:sec[i+1], sec[j]:sec[j+1]
                #             ].add(var_noise_matrix)
                if not j == i:
                    # transpose
                    Œ£ = Œ£.at[sec[j] : sec[j + 1], sec[i] : sec[i + 1]].set(
                        jnp.transpose(Œ£[sec[i] : sec[i + 1], sec[j] : sec[j + 1]])
                    )
        return Œ£

    def calculate_K_asymmetric(self, train_pts, test_pts, Ks):
        """Compute asymmetric part of covariance matrix (mixed_K)"""
        rt_num = len(test_pts)
        r_num = len(train_pts)

        rt = []
        sect = np.zeros(rt_num + 1, dtype="int")
        for i, x in enumerate(test_pts):
            rt.append(x)
            sect[i + 1 :] += len(x)

        r = []
        sec = np.zeros(r_num + 1, dtype="int")
        for i, x in enumerate(train_pts):
            r.append(x)
            sec[i + 1 :] += len(x)
        Œ£ = jnp.zeros((sect[rt_num], sec[r_num]))
        for i in range(rt_num):
            for j in range(r_num):
                Œ£ = Œ£.at[sect[i] : sect[i + 1], sec[j] : sec[j + 1]].set(
                    Ks[i][j](rt[i], r[j])
                )
        return Œ£

    def trainingFunction_all(self, Œ∏, *args):
        """Returns minus log-likelihood given Kernel hyperparamters Œ∏ and training data args
        args = velocity position, velocity average, velocity values,
            force position, force average, force values,
            jiggle parameter
        """
        # r,Œº,f,œµ=args
        r, Œº, f, œµ = args
        r_num = len(r)
        for i in range(r_num):
            if i == 0:
                Œ¥y = jnp.array(f[i] - Œº[i])
            else:
                Œ¥y = jnp.concatenate([Œ¥y, f[i] - Œº[i]], 0)
        Œ£ = self.trainingK_all(Œ∏, r)
        return self.logpGP(Œ¥y, Œ£, œµ, approx_non_pd=self.approx_non_pd)

    def predictingFunction_all(self, Œ∏, *args):
        """Returns conditional posterior average and covariance matrix given Kernel hyperparamters Œ∏  and test and training data
        args = test velocity position, test velocity average,
            training velocity position, training velocity average, training velocity values
            training force position, training force average, training force values
            jiggle parameter

        Returns
        -----------------
        Œºpost=[Œºux,Œºuy,Œºp]
        Œ£post=[Œ£ux,Œ£uy,Œ£p]
        """
        r_test, Œº_test, r_train, Œº, f_train, œµ = args
        nb = 0
        for r in r_train:
            nb += len(r)
        Œ£bb = self.trainingK_all(Œ∏, r_train)
        Œ£ab = self.mixedK_all(Œ∏, r_test, r_train)
        Œ£aa = self.testK_all(Œ∏, r_test)
        for i in range(len(r_train)):
            if i == 0:
                Œ¥fb = jnp.array(f_train[i] - Œº[i])
            else:
                Œ¥fb = jnp.concatenate([Œ¥fb, f_train[i] - Œº[i]])
                # create single training array, with velocities and forces (second derivatives)
        #         print(f'Œ¥y={Œ¥y}')
        #         print(f'Œ£={Œ£}')
        Œºposts, Œ£posts = self.postGP(
            Œ¥fb, Œ£aa, Œ£ab, Œ£bb, œµ, approx_non_pd=self.approx_non_pd
        )
        # seperate Œºpost,Œ£post to 3 section (ux,uy,p)
        sec0 = 0
        sec1 = 0
        Œºpost = []
        Œ£post = []
        for i in range(len(r_test)):
            sec1 += len(r_test[i])
            Œºpost.append(Œºposts[sec0:sec1])
            Œ£post.append(Œ£posts[sec0:sec1, sec0:sec1])
            sec0 += len(r_test[i])
            # ‰∏ÄÂøúËß£Ê±∫„Å°„Çá„Å£„Å®ÁñëÂïèÊÆã„Çã
            Œºpost[i] += Œº_test[i]
        return Œºpost, Œ£post

    def trainingK_all(self):
        raise NotImplementedError

    def mixedK_all(self):
        raise NotImplementedError

    def testK_all(self):
        raise NotImplementedError
