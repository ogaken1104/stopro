import jax
import jax.numpy as jnp
import numpy as np
from jax import grad, jit, vmap

from stopro.GP.kernels import define_kernel


@jit
def logpGP(Œ¥f, Œ£, œµ):
    """Compute minus log-likelihood of observing Œ¥f = f - <f>, for GP with covariance matrix Œ£"""
    n = len(Œ¥f)
    # jiggle parameter to improve numerical stability of cholesky decomposition
    noise = jnp.ones_like(Œ¥f)*œµ
    L = jnp.linalg.cholesky(Œ£ + jnp.diag(noise))
#     diffs=Œ£-np.dot(L,L.transpose())
#     diff=np.linalg.norm(diffs)
    v = jnp.linalg.solve(L, Œ¥f)
    return (0.5*jnp.dot(v, v) + jnp.sum(jnp.log(jnp.diag(L))) + 0.5*n*jnp.log(2.0*jnp.pi))


@jit
def postGP(Œ¥fb, Kaa, Kab, Kbb):
    """Compute posterior average and covariance from conditional GP p(fa | xa, xb, fb)
    [fa,fb] ~ ùí©([Œº_fa, Œº_fb], [[Kaa, Kab],[Kab^T, Kbb]])])
    fa|fb   ~ ùí©(Œºf + Kab Kbb \ (fb - Œº_fb) , Kaa - Kab Kbb \ Kab^T)
    """
    L = jnp.linalg.cholesky(Kbb)

    # Œ± = K \ Œ¥ f = L^t \ (L | Œ¥ f)
    Œ± = jnp.linalg.solve(L.transpose(), jnp.linalg.solve(L, Œ¥fb))

    # Œºpost - Œº(x*) = Kab Kbb \ Œ¥f(x) = Kab . Œ±
    Œºpost = jnp.dot(Kab, Œ±)

    # Kpost = Kaa - Kab Kbb | Kab^T
    #       = Kaa - W
    # W_ij  = v_i . v_j
    # v_i   = (L | c_i) ; c_i the i-th column of Kba, i-th row of Kab
    V = jnp.array([jnp.linalg.solve(L, c)
                   for c in Kab])  # V = [v_1, v_2, ... ]^t
    Kpost = Kaa - jnp.einsum('ik,jk->ij', V, V)
    return Œºpost, Kpost  # note should add Œº(x*) to average


def gp_1D_pbc(model_param, lbox):
    kernel_type = model_param['kernel_type']
    kernel_form = model_param['kernel_form']
    distance_func = model_param['distance_func']
    def outermap(f): return vmap(
        vmap(f, in_axes=(None, 0, None)), in_axes=(0, None, None))

    Kernel = define_kernel(kernel_type, kernel_form,
                           input_dim=1, distance_func=distance_func)
    # define kernel for use
    # define operators
    def L(f, i): return grad(grad(f, i), i)  # L = d^2 / dx^2
    _L1K = L(Kernel, 1)                     # (L_2 K)(x*,x')
    _L0K = L(Kernel, 0)
    _LLK = L(_L1K, 0)

    K = jit(outermap(Kernel))
    L0K = jit(outermap(_L0K))
    L1K = jit(outermap(_L1K))
    LLK = jit(outermap(_LLK))

    @jit
    def trainingK_all(Œ∏, train_pts):
        """
        Args :
          Œ∏  : kernel hyperparameters
         args: training points r_ux,r_uy,r_p,r_fx,r_fy,r_div
        """
        r_num = len(train_pts)
    #     print(Œ∏,r_num)
        Œ∏yy = Œ∏
        sec = np.zeros(r_num+1, dtype='int')
        r = []
        for i, x in enumerate(train_pts):
            r.append(x)
            sec[i+1:] += len(x)

        def Kyy(r, rp): return K(r, rp, Œ∏yy)
        def Kyly(r, rp): return L1K(r, rp, Œ∏yy)
        def Klyly(r, rp): return LLK(r, rp, Œ∏yy)
        def Kypbcy(r, rp): return K(r, rp+lbox, Œ∏yy) - K(r, rp, Œ∏yy)
        def Klypbcy(r, rp): return L0K(r, rp+lbox, Œ∏yy) - L0K(r, rp, Œ∏yy)
        def Kpbcypbcy(r, rp): return K(r+lbox, rp+lbox, Œ∏yy) - \
            K(r+lbox, rp, Œ∏yy) - K(r, rp+lbox, Œ∏yy) + K(r, rp, Œ∏yy)

        Ks = [
            [Kyy, Kyly, Kypbcy],
            [Klyly, Klypbcy],
            [Kpbcypbcy]
        ]

        Œ£ = jnp.zeros((sec[r_num], sec[r_num]))
        for i in range(r_num):
            for j in range(i, r_num):
                # upper triangular matrix
                Œ£ = Œ£.at[sec[i]:sec[i+1], sec[j]:sec[j+1]
                         ].set(Ks[i][j-i](r[i], r[j]))
                if not j == i:
                    # transpose
                    Œ£ = Œ£.at[sec[j]:sec[j+1], sec[i]:sec[i+1]
                             ].set(jnp.transpose(Œ£[sec[i]:sec[i+1], sec[j]:sec[j+1]]))
        return Œ£

    @jit
    def mixedK_all(Œ∏, test_pts, train_pts):
        rt_num = len(test_pts)
        r_num = len(train_pts)
        Œ∏yy = Œ∏

        rt = []
        sect = np.zeros(rt_num+1, dtype='int')
        for i, x in enumerate(test_pts):
            rt.append(x)
            sect[i+1:] += len(x)

        r = []
        sec = np.zeros(r_num+1, dtype='int')
        for i, x in enumerate(train_pts):
            r.append(x)
            sec[i+1:] += len(x)

        def Kyy(r, rp): return K(r, rp, Œ∏yy)
        def Kyly(r, rp): return L1K(r, rp, Œ∏yy)
        def Kypbcy(r, rp): return K(r, rp+lbox, Œ∏yy) - K(r, rp, Œ∏yy)

        Ks = [
            [Kyy, Kyly, Kypbcy]
        ]
        Œ£ = jnp.zeros((sect[rt_num], sec[r_num]))
        for i in range(rt_num):
            for j in range(r_num):
                Œ£ = Œ£.at[sect[i]:sect[i+1], sec[j]
                    :sec[j+1]].set(Ks[i][j](rt[i], r[j]))
        return Œ£

    @jit
    def testK_all(Œ∏, r_test):
        rt_num = len(r_test)
        Œ∏yy = Œ∏
        rt = r_test
        sect = np.zeros(rt_num+1, dtype='int')
        for i, x in enumerate(r_test):
            sect[i+1:] += len(x)

        def Kyy(r, rp): return K(r, rp, Œ∏yy)
        Œ£ = jnp.zeros((sect[-1], sect[-1]))
        Ks = [
            [Kyy],
        ]
        for i in range(rt_num):
            for j in range(i, rt_num):
                # upper triangular matrix
                Œ£ = Œ£.at[sect[i]:sect[i+1], sect[j]
                    :sect[j+1]].set(Ks[i][j-i](rt[i], rt[j]))
                if not j == i:
                    # transpose
                    Œ£ = Œ£.at[sect[j]:sect[j+1], sect[i]:sect[i+1]
                             ].set(jnp.transpose(Œ£[sect[i]:sect[i+1], sect[j]:sect[j+1]]))
        return Œ£

    def trainingFunction_all(Œ∏, *args):
        """Returns minus log-likelihood given Kernel hyperparamters Œ∏ and training data args
        args = velocity position, velocity average, velocity values, 
               force position, force average, force values, 
               jiggle parameter
        """
        # r,Œº,f,œµ=args
        r, Œº, f, œµ = args
        r_num = len(r)
        for i in range(r_num):
            if i == 0:
                Œ¥y = jnp.array(f[i]-Œº[i])
            else:
                Œ¥y = jnp.concatenate([Œ¥y, f[i]-Œº[i]], 0)
        Œ£ = trainingK_all(Œ∏, r)
        return logpGP(Œ¥y, Œ£, œµ)

    def predictingFunction_all(Œ∏, *args):
        """Returns conditional posterior average and covariance matrix given Kernel hyperparamters Œ∏  and test and training data
        args = test velocity position, test velocity average,
               training velocity position, training velocity average, training velocity values
               training force position, training force average, training force values
               jiggle parameter

        Returns
        -----------------
        Œºpost=[Œºux,Œºuy,Œºp]
        Œ£post=[Œ£ux,Œ£uy,Œ£p]
        """
        r_test, Œº_test, r_train, Œº, f_train, œµ = args
        nb = 0
        for r in r_train:
            nb += len(r)
        Œ£bb = trainingK_all(Œ∏, r_train) + jnp.diag(jnp.ones(nb)*œµ)
        Œ£ab = mixedK_all(Œ∏, r_test, r_train)
        Œ£aa = testK_all(Œ∏, r_test)
        for i in range(len(r_train)):
            if i == 0:
                Œ¥fb = jnp.array(f_train[i]-Œº[i])
            else:
                Œ¥fb = jnp.concatenate([Œ¥fb, f_train[i]-Œº[i]])
                # create single training array, with velocities and forces (second derivatives)
#         print(f'Œ¥y={Œ¥y}')
#         print(f'Œ£={Œ£}')
        Œºposts, Œ£posts = postGP(Œ¥fb, Œ£aa, Œ£ab, Œ£bb)
        # seperate Œºpost,Œ£post to 3 section (ux,uy,p)
        sec0 = 0
        sec1 = 0
        Œºpost = []
        Œ£post = []
        for i in range(len(r_test)):
            sec1 += len(r_test[i])
            Œºpost.append(Œºposts[sec0:sec1])
            Œ£post.append(Œ£posts[sec0:sec1, sec0:sec1])
            sec0 += len(r_test[i])
            # ‰∏ÄÂøúËß£Ê±∫„Å°„Çá„Å£„Å®ÁñëÂïèÊÆã„Çã
            Œºpost[i] += Œº_test[i]
        return Œºpost, Œ£post

    return jit(trainingFunction_all), jit(predictingFunction_all)
